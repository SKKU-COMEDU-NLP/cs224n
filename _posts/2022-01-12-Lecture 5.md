---
title: Lecture 5 - Language Models and RNNs
author: 최재석
date: 2022-01-19
category: Lecture Note
layout: post
use_math: true
---

| 작성자 | 작성일 |
| --- | --- |
| 최재석 | 2022.01.25 |

### parsing을 할 때의 문제점 3가지

**1. sparse**

**2. incomplete**

**3. expensive computation**

훈련 데이터의 features를 학습하는데 있어서 기존의 방식은 위의 문제점들이 존재했다.

그래서 이걸 써보자! 하고 도입한게 **Neural Approach** 이다.

### Neural Dependency Parser Model Architecture

![5주차_1.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_1.png)
<img width="805" alt="5주차_1" src="https://user-images.githubusercontent.com/66375489/151004402-97d2fda4-9183-42cb-a685-a232ee07f197.PNG">

위와 같이 구조가 이루어져 있다고 한다

### Graph-based parser

![5주차_2.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_2.png)
<img width="803" alt="5주차_2" src="https://user-images.githubusercontent.com/66375489/151004477-5827a331-350a-4612-9c36-c6e6af28b470.PNG">

기존의 parser를 대체하는 방식으로, 그래프에 기반하여 분석하는 방식이다.

- 단어를 2개씩 쌍을 지어서, 서로 얼마나 의존적인지를 score를 매긴 것이다.
- 위의 문장의 경우, big이라는 단어는 cat 단어와 의존적이므로 가장 점수가 높다. 이처럼 단어를 쌍을 짓고 스코어를 매기는 그래프를 형성하는 것이 graph-based parse이다.

## A bit more about neural networks

### Regularization - L2 Regularization

![5주차_4.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_4.png)
<img width="802" alt="5주차_4" src="https://user-images.githubusercontent.com/66375489/151004567-68813a7b-cd1c-4d8b-bee4-b700a299658a.PNG">

L2 규제. 모델이 테스트 데이터에 과적합(overfitting) 되는 것을 막기 위해 사용한다. 

J(θ)는 loss function. 뒤에 람다가 계수로 붙은 식을 더해줌으로써 로스값이 0이 되지 않도록 해준다. 

- 이 L2 규제에 대한 classical view(고전적인 시각): 고전적인 시각에서는, 과적합이 발생하면 모델의 학습이 잘못 되었다고 생각했다. 왜냐하면 트레이닝 데이터에서는 loss 값이 작은데, 실제 테스트 데이터에서는 loss 값이 크게 나왔기 때문.
- 비유하자면, 우리 모델이 연습했던 기출문제는 잘 푸는데 막상 시험장 보내 놓으면 애가 성적이 영 안나오는 상황이라 생각했기 때문이다.

하지만 현재의 시각에선, 트레이닝 데이터에 과적합되는 것이 막 엄청 큰 문제는 아니라고 한다.

### Dropout

![5주차_5.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_5.png)
<img width="802" alt="5주차_5" src="https://user-images.githubusercontent.com/66375489/151004588-1fa95623-e180-4900-8aba-026be5d3ea7b.PNG">

- Dropout. 이 또한 모델이 훈련 데이터에 과적합 되는 현상을 막기 위해 고안된 방법이다
- 슬라이드에서 확인할 수 있듯, 2012년~2014년에 논문이 발표되면서 고안된 방법이다. 10년? 정도 밖에 안된 나름 최신이라면 최신인 방법인 셈.
- 기본적인 원리를 정리하자면, 특정 배치에서 훈련을 진행할 때, 특정 뉴런을 껐다 켰다 하면서 훈련에 제외했다 참가시켰다를 진행하는 것이다.
- 이런 방법은 특정 뉴런(가중치나 바이어스가 큰 뉴런)이 훈련에서 큰 영향을 미치는 것을 어느정도 방지할 수 있으며, 과적합이 일어나더라도 전체 뉴런이 아니라 몇몇 뉴런에 쪼끔씩 나도록 해서 훈련을 모두 마치고 나면 타협할 수 있는 수준의 과적합이 나도록 한다.
- 비유하자면, 고등학교 축구(or 농구)팀 에이스를 가끔씩은 훈련에서 빼주는 느낌이랄까? 고루고루 대부분의 선수들이 탄탄해야 팀이 강해지듯, 딥러닝 모델에서도 가중치나 바이어스가 큰 모델에서 시간을 다 잡아먹지 못하도록 한번씩 포함했다가 제외했다가 하며 학습을 진행한다.

### Vectorization

![5주차_6.PNG]<img width="802" alt="5주차_6" src="https://user-images.githubusercontent.com/66375489/151004601-5a2a00e1-7c9b-4265-8fb5-83f14bca3297.PNG">
(Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_6.png)

- 단어를 벡터화하는 vectorization.
- 요약하자면, for문을 써서 저장할 때보다 vector를 다루는 matrix를 써서 저장하는게 더 빠르다는 이야기이다.
- 딥러닝 단계로 나아가면 데이터가 굉장히 많고 용량도 크고 학습 시간도 그만큼 오래 걸리는데, 때문에 word vector를 다루는 시간이 짧으면 짧을수록 좋다. 이 때 matrix(행렬)을 쓰는게 시간이 덜 걸린다는 것이다.

### Non-linearities, Old and New

![5주차_7.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_7.png)
<img width="803" alt="5주차_7" src="https://user-images.githubusercontent.com/66375489/151004672-50870c78-67d5-43d0-86d3-fa9957373cc1.PNG">

- Activation Function(활성함수) 발전 과정.
- 활성함수란, 해당 뉴런을 활성화해서 다음 층에 신호를 쏴줄지 말지 결정하는 함수이다. 제일 왼쪽에 있는 logistic 함수를 예로 들자면, z(z = wx + b의 정방향 계산으로 연산된 값)을 활성 함수에 집어 넣고, 이게 0.5 (threshold라고 표현한다)를 이상이면 뉴런을 활성화하고, 0.5 미만이면 비활성화 하는 것이다.
- 그런데 문제는, 모델의 층이 많아지고(히든 레이어가 겹겹이 쌓이고), 데이터가 워낙 많아지다보면 데이터의 feature가 희미해지는 문제가 발생하게 된다.
- 이는 시그모이드 함수의 구조적인 문제에서 기인하는데, 입력되는 z값이 어떤 값이든 0~1 사이의 값으로 f(z)가 계산되기 때문이다. 그래서 다른 활성화 함수가 필요한 것이다.
- 현재 제일 무난하게 많이 쓰이는 것은 ReLU 함수이다. 0보다 작으면 아예 0, 0보다 크면 z값을 그대로 전달하는 방식이다.

### Parameter Initialization

![5주차_8.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_8.png)
<img width="804" alt="5주차_8" src="https://user-images.githubusercontent.com/66375489/151004691-5234aeba-5bef-4afc-bf5f-9f20649823ed.PNG">

- 초기에 parameter를 초기화 하는 것을 말한다.
- 정방향 계산을 할 때에, z = wx + b 식을 계산해야 하는데 가중치인 w가 초기화되어있지 않으면 연산을 진행할 수 없다. 이를 랜덤하게, 적당히 작은 수로 초기화 하는 과정을 이야기 한다.

### Optimizers

![5주차_9.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_9.png)
<img width="800" alt="5주차_9" src="https://user-images.githubusercontent.com/66375489/151004709-1db6aafb-e80c-4407-815d-ec7549f8d6f0.PNG">

- loss 값을 줄이고, 어떻게 내가 원하는 결과값에 가까워질 수 있는지를 조절하는 것.
- 제일 국룰인 방법이고 자주 쓰는게 SGD(Stochastic Gradient Descent, 확률적 경사하강법)이고, 그 외에 다른 여러가지 optimizer들이 있다.

### Learning Rates

![5주차_10.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_10.png)
<img width="803" alt="5주차_10" src="https://user-images.githubusercontent.com/66375489/151004726-53c2ea6f-689b-452f-acb8-f51c6a100b4b.PNG">

- 우리말로는 학습률 이라고 하는 것인데, 비유하자면 걸음 보폭? 정도로 할 수 있겠다.
- 우리가 돌로 된 징검다리를 건너는데, 모든 돌에서 성큼성큼 걸어나가면 미끄러질 수도 있다. 이를 방지하기 위해서 돌다리도 두들겨보고 건넌다는 마인드로 종종걸음으로 다리를 건너면 안전할텐데, 학습률이 그런 느낌이다.
- 학습을 진행하다가 갑자기 값이 팍 하고 튀는 경우가 있는데, 그 튀어버린 값이 다음 계산으로 넘어가면 inf나 Nan이 뜨는 경우가 왕왕있다. 이런 경우를 막기 위해 학습률을 설정해서 좀 작게 만들어서 연산을 지속할 수 있도록 하는 것.

### Language Modeling

![5주차_11.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_11.png)
<img width="803" alt="5주차_11" src="https://user-images.githubusercontent.com/66375489/151004733-1db0a974-a7c6-4dfc-96eb-5f2d9acc1f2b.PNG">

- Language Modeling. 어떤 단어 다음에 어떤 단어가 올지 예측하는 것을 말한다.
- sequence of words를 벡터로 나타냈을 때, 마지막 단어 다음에 어떤 단어가 올지 예측하는 것이고, 이는 조건부 확률을 통해 나타내게 된다.
- 이는 실생활에서도 적용되는데, 아이폰 자동완성 기능 (대부분 사람들은 꺼놓는 기능이지만) 이라거나, 검색어 자동완성 기능 등이 language modeling이다.

### n-grams

![5주차_12.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_12.png)
<img width="723" alt="5주차_12" src="https://user-images.githubusercontent.com/66375489/151004738-1167d311-45a5-45e4-a69e-641b1e904655.PNG">

- n-gram language models. 한 문장을 분석하는데, window(창문) 사이즈를 어느 정도로 할 것이느냐를 말한다.
- 문장을 단어 단위로 쪼개어서 벡터화를 시키면, 이제 이걸 몇개씩 묶어서 볼 것이느냐가 중요한 문제가 된다. 띄어쓰기 단위로 하나씩 볼지, 아니면 두개씩 묶어서 볼지, 혹은 그 이상으로 볼지...
- 이 묶음을 n-gram이라고 한다. uni, bi, tri, 4, ... 단위로 나타낸다. n개의 연속적인 단어 집합이라고 생각하면 된다.

### n-gram language model의 문제점

![5주차_13.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_13.png)
<img width="803" alt="5주차_13" src="https://user-images.githubusercontent.com/66375489/151004750-cbc4414a-5e83-41fc-babc-f83c83ddb5c1.PNG">

- n-gram 방식의 경우, 사전 데이터(단어 임베딩 등)에 그 경우가 없으면 어떡하냐! 라는 문제가 발생한다.
- 앞서 조건부 확률을 따진다고 했는데, bi-gram에서 word1 word2 (?)의 경우라면 word1 word2의 경우를 따지기 좋지만, 만약 4-gram에서 word1 word word3 word4 (?)의 경우라면, 저 순서와 동일하게 배열된 데이터가 없을 수도 있다는 것이다. 즉, 통계적으로 뭐가 많이 나오더라~ 라는 연산 자체가 안된다는 것.
- 따라서 n-gram 방식에서는 n을 5보다 크게 두진 않는다고 한다.

### RNN Language Model

![5주차_14.PNG](Lecture%205%20-%20Language%20Models%20and%20RNNs%2004767d2c44444db48909bb6bff5962fd/5%EC%A3%BC%EC%B0%A8_14.png)
<img width="803" alt="5주차_14" src="https://user-images.githubusercontent.com/66375489/151004767-76036b63-4c14-4152-b68d-9a8af7140eb9.PNG">
